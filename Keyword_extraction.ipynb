{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of RAKE\n",
    "(Based on: https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source of text:\n",
    "#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "\n",
    "Text = \"Compatibility of systems of linear constraints over the set of natural numbers. \\\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and \\\n",
    "nonstrict inequations are considered. \\\n",
    "Upper bounds for components of a minimal set of solutions and \\\n",
    "algorithms of construction of minimal generating sets of solutions for all \\\n",
    "types of systems are given. \\\n",
    "These criteria and the corresponding algorithms for constructing \\\n",
    "a minimal supporting set of solutions can be used in solving all the \\\n",
    "considered types of systems and systems of mixed types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw input text is cleaned off non-printable characters (if any) and turned into lower case.\n",
    "The processed input text is then tokenized using NLTK library functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['compatibility', 'of', 'systems', 'of', 'linear', 'constraints', 'over', 'the', 'set', 'of', 'natural', 'numbers', '.', 'criteria', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equations', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bounds', 'for', 'components', 'of', 'a', 'minimal', 'set', 'of', 'solutions', 'and', 'algorithms', 'of', 'construction', 'of', 'minimal', 'generating', 'sets', 'of', 'solutions', 'for', 'all', 'types', 'of', 'systems', 'are', 'given', '.', 'these', 'criteria', 'and', 'the', 'corresponding', 'algorithms', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solutions', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'types', 'of', 'systems', 'and', 'systems', 'of', 'mixed', 'types', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text) #filter funny characters, if any.\n",
    "    return text\n",
    "\n",
    "Cleaned_text = clean(Text)\n",
    "\n",
    "text = word_tokenize(Cleaned_text)\n",
    "\n",
    "print \"Tokenized Text: \\n\"\n",
    "print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is again used for <b>POS tagging</b> the input text.\n",
    "\n",
    "\n",
    "Description of POS tags: \n",
    "\n",
    "\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n",
      "[('compatibility', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('of', 'IN'), ('linear', 'JJ'), ('constraints', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('numbers', 'NNS'), ('.', '.'), ('criteria', 'NNS'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'NN'), ('equations', 'NNS'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bounds', 'NNS'), ('for', 'IN'), ('components', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('and', 'CC'), ('algorithms', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('sets', 'NNS'), ('of', 'IN'), ('solutions', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criteria', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithms', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('of', 'IN'), ('mixed', 'JJ'), ('types', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print \"Tokenized Text with POS tags: \\n\"\n",
    "print POS_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\n",
    "In lemmatization different grammatical counterparts of a word will be replaced by single\n",
    "basic lemma. For example, 'glasses' may be replaced by 'glass'. \n",
    "\n",
    "Details about lemmatization: \n",
    "    \n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['compatibility', 'of', 'system', 'of', 'linear', 'constraint', 'over', 'the', 'set', 'of', 'natural', 'number', '.', 'criterion', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equation', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bound', 'for', 'component', 'of', 'a', 'minimal', 'set', 'of', 'solution', 'and', 'algorithm', 'of', 'construction', 'of', 'minimal', 'generating', 'set', 'of', 'solution', 'for', 'all', 'type', 'of', 'system', 'are', 'given', '.', 'these', 'criterion', 'and', 'the', 'corresponding', 'algorithm', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solution', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'type', 'of', 'system', 'and', 'system', 'of', 'mixed', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print \"Text tokens after lemmatization of adjectives and nouns: \\n\"\n",
    "print lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>lemmatized text</b> is <b>POS tagged</b> here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('compatibility', 'NN'), ('of', 'IN'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('constraint', 'NN'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('number', 'NN'), ('.', '.'), ('criterion', 'NN'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'JJ'), ('equation', 'NN'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bound', 'NN'), ('for', 'IN'), ('component', 'NN'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('and', 'CC'), ('algorithm', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('for', 'IN'), ('all', 'DT'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criterion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithm', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('and', 'CC'), ('system', 'NN'), ('of', 'IN'), ('mixed', 'JJ'), ('type', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "print \"Lemmatized text with POS tags: \\n\"\n",
    "print POS_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here\n",
    "considered as a <b>stopword</b> (non-content). This is based on the assumption that usually keywords are noun,\n",
    "adjectives or gerunds. \n",
    "\n",
    "Punctuations are added to the stopword list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we remove the aforementioned stopwords, still some extremely common nouns, adjectives or gerunds may\n",
    "remain which are very bad candidates for being keywords (or part of it). \n",
    "\n",
    "An external file constituting a long list of stopwords is loaded and all the words are added with the previous\n",
    "stopwords to create the final list 'stopwords-plus' which is then converted into a set. \n",
    "\n",
    "(Source of stopwords data: https://www.ranks.nl/stopwords)\n",
    "\n",
    "Stopwords-plus constitute the sum total of all stopwords and potential phrase-delimiters. The contents of this\n",
    "set will be used to partition the lemmatized text into phrases. \n",
    "\n",
    "Phrases should constitute a group of consecutively occuring words that has no member from stopwords_plus in\n",
    "between. Example: \"Neural Network\".\n",
    "    \n",
    "Each phrase is a <b>keyword candidate</b>. \n",
    "    \n",
    "There are some exceptions, that is, there are some possible cases where a good keyword candidate may contain \n",
    "stopword in between. Example: \"Word of Mouth\". \n",
    "    \n",
    "But, for simplicity's sake I will pretend here that such exceptions do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "#Stopwords_plus contain total set of all stopwords and phrase delimiters that\n",
    "#will be used for partitioning the text into phrases (candidate keywords)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases are generated by partitioning the lemmatized text using the members of stopwords_plus \n",
    "as delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases: \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['compatibility'], ['system'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['solution'], ['type'], ['system'], ['criterion'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solution'], ['solving'], ['type'], ['system'], ['system'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print \"Partitioned Phrases: \\n\"\n",
    "print phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the RAKE algorithm.\n",
    "\n",
    "Frequency of each words in the list of phrases, are calculated here. \n",
    "\n",
    "The degree of each words are calculating by adding the length of all the\n",
    "phrases where the word occurs.\n",
    "\n",
    "Each word scores are caclulated by dividing degree of the word by its frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary of degree scores for each words under candidate keywords (phrases): \n",
      "\n",
      "defaultdict(<type 'int'>, {'upper': 2, 'set': 9, 'constructing': 1, 'number': 2, 'solving': 1, 'system': 5, 'compatibility': 2, 'strict': 2, 'criterion': 2, 'type': 4, 'minimal': 8, 'supporting': 3, 'generating': 3, 'corresponding': 2, 'linear': 5, 'diophantine': 3, 'component': 1, 'bound': 2, 'nonstrict': 2, 'inequations': 4, 'natural': 2, 'algorithm': 3, 'constraint': 2, 'equation': 3, 'solution': 3, 'construction': 1, 'mixed': 2})\n",
      "\n",
      "Dictionary of frequencies for each words under candidate keywords (phrases): \n",
      "\n",
      "defaultdict(<type 'int'>, {'upper': 1, 'set': 4, 'constructing': 1, 'number': 1, 'solving': 1, 'system': 5, 'compatibility': 2, 'strict': 1, 'criterion': 2, 'type': 3, 'minimal': 3, 'supporting': 1, 'generating': 1, 'corresponding': 1, 'linear': 2, 'diophantine': 1, 'component': 1, 'bound': 1, 'nonstrict': 1, 'inequations': 2, 'natural': 1, 'algorithm': 2, 'constraint': 1, 'equation': 1, 'solution': 3, 'construction': 1, 'mixed': 1})\n",
      "\n",
      "Dictionary of word scores for each words under candidate keywords (phrases): \n",
      "\n",
      "defaultdict(<type 'float'>, {'upper': 2.0, 'set': 2.25, 'constructing': 1.0, 'number': 2.0, 'solving': 1.0, 'system': 1.0, 'compatibility': 1.0, 'strict': 2.0, 'criterion': 1.0, 'type': 1.3333333333333333, 'minimal': 2.6666666666666665, 'supporting': 3.0, 'generating': 3.0, 'corresponding': 2.0, 'linear': 2.5, 'diophantine': 3.0, 'component': 1.0, 'bound': 2.0, 'nonstrict': 2.0, 'inequations': 2.0, 'natural': 2.0, 'algorithm': 1.5, 'constraint': 2.0, 'equation': 3.0, 'solution': 1.0, 'construction': 1.0, 'mixed': 2.0})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from __future__ import division\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "degree = defaultdict(int)\n",
    "word_score = defaultdict(float)\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    for word in phrase:\n",
    "        frequency[word]+=1\n",
    "        degree[word]+=len(phrase)\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "            \n",
    "for word in vocabulary:\n",
    "    word_score[word] = degree[word]/frequency[word]\n",
    "\n",
    "print \"Dictionary of degree scores for each words under candidate keywords (phrases): \\n\"\n",
    "print degree\n",
    "print \"\\nDictionary of frequencies for each words under candidate keywords (phrases): \\n\"\n",
    "print frequency\n",
    "print \"\\nDictionary of word scores for each words under candidate keywords (phrases): \\n\"\n",
    "print word_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrase scores are calculated by adding individual scores of each of the words\n",
    "which form the members of the phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of candidate keyword 'compatibility': 1.0\n",
      "Score of candidate keyword 'system': 1.0\n",
      "Score of candidate keyword 'linear constraint': 4.5\n",
      "Score of candidate keyword 'set': 2.25\n",
      "Score of candidate keyword 'natural number': 4.0\n",
      "Score of candidate keyword 'criterion': 1.0\n",
      "Score of candidate keyword 'compatibility': 1.0\n",
      "Score of candidate keyword 'system': 1.0\n",
      "Score of candidate keyword 'linear diophantine equation': 8.5\n",
      "Score of candidate keyword 'strict inequations': 4.0\n",
      "Score of candidate keyword 'nonstrict inequations': 4.0\n",
      "Score of candidate keyword 'upper bound': 4.0\n",
      "Score of candidate keyword 'component': 1.0\n",
      "Score of candidate keyword 'minimal set': 4.91667\n",
      "Score of candidate keyword 'solution': 1.0\n",
      "Score of candidate keyword 'algorithm': 1.5\n",
      "Score of candidate keyword 'construction': 1.0\n",
      "Score of candidate keyword 'minimal generating set': 7.91667\n",
      "Score of candidate keyword 'solution': 1.0\n",
      "Score of candidate keyword 'type': 1.33333\n",
      "Score of candidate keyword 'system': 1.0\n",
      "Score of candidate keyword 'criterion': 1.0\n",
      "Score of candidate keyword 'corresponding algorithm': 3.5\n",
      "Score of candidate keyword 'constructing': 1.0\n",
      "Score of candidate keyword 'minimal supporting set': 7.91667\n",
      "Score of candidate keyword 'solution': 1.0\n",
      "Score of candidate keyword 'solving': 1.0\n",
      "Score of candidate keyword 'type': 1.33333\n",
      "Score of candidate keyword 'system': 1.0\n",
      "Score of candidate keyword 'system': 1.0\n",
      "Score of candidate keyword 'mixed type': 3.33333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index=0\n",
    "phrase_scores = np.zeros((len(phrases)),dtype=np.float32)\n",
    "keywords = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    for word in phrase:\n",
    "        phrase_scores[index] += word_score[word]\n",
    "    index+=1\n",
    "\n",
    "for i in xrange(0,len(phrases)):\n",
    "    \n",
    "    keyword=''\n",
    "    for word in phrases[i]:\n",
    "        keyword += str(word)+\" \"\n",
    "    \n",
    "    keyword = keyword.strip()\n",
    "    keywords.append(keyword)\n",
    "    \n",
    "    print \"Score of candidate keyword '\"+keywords[i]+\"': \"+str(phrase_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the phrase score ndarray is then sorted in descending order in terms of\n",
    "the score values.\n",
    "The index corresponds to the location of the concerned phrase in phrases list.\n",
    "So by getting the sorted order of the index, we also get the sorted order of the phrases.\n",
    "Each phrase can be considered as a <b>candidate keyword</b>. \n",
    "We can then simply choose the top n highest scoring candidate keywords and present them as\n",
    "the final exctracted keywords for the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "linear diophantine equation,  minimal generating set,  minimal supporting set,  minimal set,  linear constraint,  natural number,  strict inequations,  nonstrict inequations,  upper bound,  corresponding algorithm, \n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print \"Keywords:\\n\"\n",
    "\n",
    "for i in xrange(0,keywords_num):\n",
    "    print str(keywords[sorted_index[i]])+\", \","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
